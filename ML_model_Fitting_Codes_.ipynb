{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5svs8FO0CXuhyJ4M+pGUR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datagrad/1.ML/blob/main/ML_model_Fitting_Codes_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming that `X_train` and `y_train` are training features and labels, respectively.\n",
        "\n",
        "### 1. Linear Regression\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize the model\n",
        "linear_reg = LinearRegression(normalize=True)\n",
        "# Fit the model\n",
        "linear_reg.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "**Parameter Importance:**\n",
        "- `normalize`: Previously, this parameter normalized the input variables before regression by subtracting the mean and dividing by the l2-norm. However, it's recommended to use `StandardScaler` for preprocessing.\n",
        "\n",
        "### 2. Logistic Regression\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the model with regularization\n",
        "logistic_reg = LogisticRegression(C=1.0, penalty='l2', solver='liblinear')\n",
        "# Fit the model\n",
        "logistic_reg.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "**Parameter Importance:**\n",
        "- `C`: Inverse of regularization strength; smaller values specify stronger regularization. Helps prevent overfitting.\n",
        "- `penalty`: Specifies the norm used in the penalization (regularization).\n",
        "- `solver`: Algorithm to use in the optimization problem. For small datasets, `'liblinear'` is a good choice.\n",
        "\n",
        "### 3. Decision Tree\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the model with depth and feature constraints\n",
        "decision_tree = DecisionTreeClassifier(max_depth=5, max_features=5)\n",
        "# Fit the model\n",
        "decision_tree.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "**Parameter Importance:**\n",
        "- `max_depth`: The maximum depth of the tree. Limits the number of nodes in the tree to prevent overfitting.\n",
        "- `max_features`: The number of features to consider when looking for the best split. Helps in reducing variance and making the model more robust.\n",
        "\n",
        "### 4. Random Forest\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the model with more options\n",
        "random_forest = RandomForestClassifier(n_estimators=100, max_features=5, max_depth=5, min_samples_split=4)\n",
        "# Fit the model\n",
        "random_forest.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "**Parameter Importance:**\n",
        "- `n_estimators`: The number of trees in the forest. More trees increase accuracy but also computational cost.\n",
        "- `max_features`: The number of features to consider when looking for the best split.\n",
        "- `max_depth`: The maximum depth of the trees.\n",
        "- `min_samples_split`: The minimum number of samples required to split an internal node. Higher values prevent creating nodes that represent too few samples, thus avoiding overfitting.\n",
        "\n",
        "### 5. XGBoost\n",
        "\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize the model with more detailed parameters\n",
        "xgboost_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_estimators=100, max_depth=5, learning_rate=0.1, min_child_weight=1)\n",
        "# Fit the model\n",
        "xgboost_model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "**Parameter Importance:**\n",
        "- `use_label_encoder`: Avoids using label encoder to prevent additional warnings.\n",
        "- `eval_metric`: Evaluation metrics for validation data, a default might vary by objective.\n",
        "- `n_estimators`: Number of gradient boosted trees. Equivalent to the number of boosting rounds.\n",
        "- `max_depth`: Maximum depth of a tree. Increasing this value will make the model more complex and likely to overfit.\n",
        "- `learning_rate`: Step size shrinkage used in update to prevents overfitting. It makes the model more robust by shrinking the weights on each step.\n",
        "- `min_child_weight`: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than `min_child_weight`, then the building process will give up further partitioning.\n",
        "\n"
      ],
      "metadata": {
        "id": "gy0GESgBg7qv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Support Vector Machines (SVM) for Classification\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize the model with kernel choice\n",
        "svm_model = SVC(kernel='linear', C=1.0)\n",
        "# Fit the model\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# kernel: Specifies the kernel type to be used in the algorithm.\n",
        "# C: Regularization parameter. The strength of the regularization is inversely proportional to C.\n",
        "```\n",
        "\n",
        "### K-Nearest Neighbors (KNN)\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Initialize the model with number of neighbors\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "# Fit the model\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# n_neighbors: Number of neighbors to use for kneighbors queries.\n",
        "```\n",
        "\n",
        "### Naive Bayes for Classification\n",
        "\n",
        "```python\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize the model\n",
        "naive_bayes_model = GaussianNB()\n",
        "# Fit the model\n",
        "naive_bayes_model.fit(X_train, y_train)\n",
        "\n",
        "# No specific parameters needed for the basic model. It assumes that the features follow a normal distribution.\n",
        "```\n",
        "\n",
        "### Gradient Boosting for Classification\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Initialize the model with learning rate and number of estimators\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1)\n",
        "# Fit the model\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# n_estimators: The number of boosting stages to be run.\n",
        "# learning_rate: Learning rate shrinks the contribution of each tree by learning_rate.\n",
        "```\n",
        "\n",
        "### Ridge Regression for Regression Tasks\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Initialize the model with regularization strength\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "# Fit the model\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# alpha: Regularization strength; must be a positive float. Regularization improves the conditioning of the problem.\n",
        "```\n",
        "\n",
        "### Lasso Regression for Regression Tasks\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Initialize the model with regularization strength\n",
        "lasso_model = Lasso(alpha=1.0)\n",
        "# Fit the model\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# alpha: Regularization strength; similarly to Ridge, it controls the amount of shrinkage.\n",
        "```\n"
      ],
      "metadata": {
        "id": "DxhLasrKjaX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are additional examples across various machine learning tasks, including ensemble methods, clustering, and dimensionality reduction techniques. Each example includes the model initialization, fitting, and a brief commentary on the parameters.\n",
        "\n",
        "### Ensemble Methods\n",
        "\n",
        "#### AdaBoost for Classification\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Initialize the model with the number of estimators\n",
        "ada_boost_model = AdaBoostClassifier(n_estimators=100)\n",
        "# Fit the model\n",
        "ada_boost_model.fit(X_train, y_train)\n",
        "\n",
        "# n_estimators: The maximum number of estimators at which boosting is terminated.\n",
        "```\n",
        "\n",
        "#### Bagging for Classification\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the model with base estimator and number of base estimators\n",
        "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10)\n",
        "# Fit the model\n",
        "bagging_model.fit(X_train, y_train)\n",
        "\n",
        "# base_estimator: The base model to ensemble. Default is Decision Tree.\n",
        "# n_estimators: The number of base estimators in the ensemble.\n",
        "```\n",
        "\n",
        "### Clustering\n",
        "\n",
        "#### K-Means Clustering\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Initialize the model with number of clusters\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "# Fit the model\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "# n_clusters: The number of clusters to form as well as the number of centroids to generate.\n",
        "```\n",
        "\n",
        "#### DBSCAN for Clustering\n",
        "\n",
        "```python\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Initialize the model with eps and min_samples\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "# Fit the model\n",
        "dbscan.fit(X_train)\n",
        "\n",
        "# eps: The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
        "# min_samples: The number of samples in a neighborhood for a point to be considered as a core point.\n",
        "```\n",
        "\n",
        "### Dimensionality Reduction\n",
        "\n",
        "#### Principal Component Analysis (PCA)\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize PCA with number of components\n",
        "pca = PCA(n_components=2)\n",
        "# Fit and transform the data\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "\n",
        "# n_components: The number of components to keep.\n",
        "```\n",
        "\n",
        "#### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "```python\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Initialize t-SNE with number of components\n",
        "tsne = TSNE(n_components=2, perplexity=30.0)\n",
        "# Fit and transform the data\n",
        "X_train_tsne = tsne.fit_transform(X_train)\n",
        "\n",
        "# n_components: The dimension of the embedded space.\n",
        "# perplexity: The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms.\n",
        "```\n",
        "\n",
        "These models cover a broad spectrum of machine learning applications, from supervised learning (both classification and regression), over ensemble methods that improve prediction robustness, to unsupervised learning techniques like clustering and dimensionality reduction, which are great for data exploration. As with any model, the choice and tuning of parameters are critical to achieving optimal performance, so experimentation and validation are key steps in the modeling process."
      ],
      "metadata": {
        "id": "BbP9HMGHjwok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmS5UzxZfGf2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Matrices Code for For Classification Models"
      ],
      "metadata": {
        "id": "qSnCUXXYh1lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Use respective Predictions code for the model and comment out other\n",
        "y_pred_1 = logistic_reg.predict(X_test)\n",
        "y_pred_1 = decision_tree.predict(X_test)\n",
        "y_pred_1 = random_forest.predict(X_test)\n",
        "y_pred_1 = xgboost_model.predict(X_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Metrics\n",
        "print(\"Decision Tree Performance:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_1))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred_1, average='binary'))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred_1, average='binary'))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred_1, average='binary'))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_1))\n",
        "\n"
      ],
      "metadata": {
        "id": "1Lpj2Ym7h747"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iAd0lVlIh7pB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NkV1JNihh7h1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9vJMVXxkh7X2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kSESaEG5h65I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the model\n",
        "logistic_reg = LogisticRegression()\n",
        "\n",
        "# Fit the model\n",
        "logistic_reg.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "LYy45eDYfUYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the model\n",
        "decision_tree = DecisionTreeClassifier()\n",
        "\n",
        "# Fit the model\n",
        "decision_tree.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "v7RSxYFEfUU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize the model\n",
        "random_forest = RandomForestClassifier(n_estimators=100, max_features = 5, max_depth = 5)  # You can adjust n_estimators\n",
        "\n",
        "# Fit the model\n",
        "random_forest.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "vPn1ghIxfURt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Initialize the model\n",
        "xgboost_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "# Fit the model\n",
        "xgboost_model.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "h-bLCIsqfUKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0PG9mnGkfUBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}